# 설명 가능한 인공지능(XAI, Explainable AI ) 이란?

AI는 자율주행, 제조업, 암 진단, 은행 고객 응대 등 활용 분야가 넓은 기술입니다. 사람보다 더 뚜렷한 성과를 내기 때문에 다가오는 미래에는 단순 노동을 넘어 전문적인 업무를 대체할 것으로 전망되고 있습니다. 

 

하지만 AI가 잘못된 판단을 하면 심각한 문제가 될 수 있습니다. 결과에 대한 근거 및 도출과정을 생략하거나, 오류 발생 시 원인을 즉시 알 수 없어서 개발자마저 AI의 문제를 파악하지 못하는 경우가 있습니다. 

 

‘어떤 과정을 통해서 결과를 도출해냈는지 설명할 수 있다면 사람들이 훨씬 더 잘 받아들이고 AI를 신뢰할 수 있지 않겠는가?’ 하는 관점에 설명 가능한 AI(eXplainable AI, XAI)가 거론되고 있는 것입니다. 

 

**설명 가능한 AI(eXplainable AI, XAI)는** 

**AI 스스로 사람이 이해할 수 있게끔 설명해줍니다!**

### **현재 머신러닝 알고리즘의 약점**

AI도 사람처럼 실수할 수 있습니다. AI가 신처럼 완벽했다면, 굳이 설명을 요구할 필요가 없었겠죠. 하지만 미세한 방해 때문에 오류를 범한다면 어떨까요? 답은 정해져있는데 의문의 결과를 도출한다면, 과연 그런 예측불허의 기술을 신뢰할 수 있을까요? 현재 AI가 직면한 약점에 대해 함께 알아보겠습니다. 

 



![img](https://blog.kakaocdn.net/dn/JlB9l/btqHq2IxH43/uGDnbLC1AAMoCNyEwNyZZK/img.png)적대적 공격(Adversarial Attack)으로 AI를 속이다



위 이미지를 볼 때 일반적인 사람은 양쪽이 판다라는 것을 알 수 있습니다. 하지만 AI는 왼쪽 사진을 보고 57.7%의 낮은 확률로 판다인 것으로 판단했습니다. 그리고 보이는 바와 같이 기존 판다 사진에 일정 수치의 노이즈를 입력했더니 AI가 99.3%의 높은 확률로 이 동물은 긴팔원숭이라는 어이없는 답을 도출했습니다. 이런 상황을 적대적 공격(Adversarial Attack)이라고 표현합니다. 이는 눈으로 구분할 수 없는 장치를 더해서 머신러닝 알고리즘에 내재하고 있는 취약점을 드러나게 하는 경우를 뜻합니다.

 



![img](https://blog.kakaocdn.net/dn/9iBxt/btqHiMmWLRf/Yh34WTOfZ9sJuJogfDGMi1/img.png)정지 표지판에 스티커를 붙혀 딥마인드를 속이다



워싱턴 대학에 있는 한 연구팀은 도로에 있는 정지 표지판에 스티커를 붙여 딥 러닝 기반 자율주행차가 ‘정지’ 신호를 ‘시속 45마일 속도 제한’ 신호로 인식하도록 오인을 유도하는 방법을 발견했습니다. 

 

스티커와 노이즈, 또는 픽셀 하나로 딥 러닝을 속일 수 있다는 것은 아직 AI가 성장할 수 있는 여지가 있다는 것입니다. [XAI에 대해 주목](http://www.aitimes.kr/news/articleView.html?idxno=14859)하고 나서부터 많은 AI 기업과 연구소들은 각양각색의 XAI를 개발하고 있습니다.

### 딥 러닝(Deep Learning)과 섈로우 러닝(Shallow Learning)으로 보는 XAI



![img](https://blog.kakaocdn.net/dn/dMUI2t/btqHq2IxTgF/TyZq5HBKXQ5Kw11Tj8I1bK/img.png)성능 vs 설명



위 이미지의 많은 학습 방법론 중에서 학습 성능이 가장 좋다고 평가받는 것은 **딥 러닝**입니다. 하지만 딥 러닝은 설명력이 부족한 ‘블랙박스’로 알려져 있습니다. 블랙박스라고 부르는 이유는 특정 결론에 이르는 과정이 불투명하고, 이유를 알기 어렵기 때문입니다. 딥 러닝은 특징 추출부터 판단까지 전부 딥 러닝 네트워크가 알아서 하므로 어떤 과정을 통해서 답을 추론했는지 알 수 없습니다. 

 

결정 트리(decision tree)는 의사결정 분석 목표에 가장 가까운 결과를 낼 수 있는 방법론입니다. 이는 학습 성능이 떨어지지만, 결과를 어떻게 도출했는지 논리적으로 설명할 수 있습니다. 결정 트리를 포함한 대부분 과거 머신러닝 방법론들은 2~3개의 계층으로 구성된 기존의 인공신경망 구조인 쉘로우 러닝(Shallow Learning)에 속하는데, 사람의 로직이 들어가 있기에 도출한 결과에 관해 설명이 가능한 것입니다. 

### **XAI를 통한 효과**



![img](https://blog.kakaocdn.net/dn/lEBvV/btqHjpSiPSU/WqwZVGVEKgeekk1creAefK/img.png)새로운 학습을 통해 설명 가능한 모델 구현 



 

XAI 사용자(데이터 분석가, 엔지니어, 결정 담당자 등)의 업무는 크게 3단계의 프로세스로 구분됩니다.

#### **XAI 업무 프로세스:** 

\1. XAI가 모델을 학습하고 과제에 대한 결론을 내린다. 

\2. 담당자는 인터페이스를 통해 XAI의 내부 모델을 파악한다. 

\3. 담당자는 AI가 제공한 결론을 통해 업무를 수행한다.

 

이 업무 프로세스는 엔지니어뿐만 아니라 일반적인 병원이나 기업의 업무 담당자에게도 적용될 수 있습니다. 다만 엔지니어에 비해 비전문가가 이해하는 수준이 다르므로 이해하기 쉬운 수준의 설명 모델이나 인터페이스를 제공해야 할 것입니다. 

 

예를 들어 의사는 암에 투병 중인 환자에게 약을 처방하기 전에 결정을 내려야 합니다. AI가 도출한 결과를 보고 의사는 그간 쌓아온 경험을 통해 최종 결정을 내려야 합니다. 여기서 AI가 최종 결과에 대한 근거와 도출 과정을 제공하지 않았다면 의사는 결론만 보고 결정을 내리기 어려울 수 있습니다. 만약 보기 쉬운 인터페이스로 AI의 과정을 볼 수 있다면, 그에 대한 데이터를 신뢰할 수 있을 것이고 약을 처방 하는데도 도움이 될 것입니다. 

### **결론**

XAI는 설명 가능한 알고리즘을 통해 최종 결과에 대한 근거를 제시합니다. 이는 기존 AI에 신뢰성을 높이기 위한 목적으로 개발되었습니다. 그리고 최근, XAI에 대한 연구와 개발이 금융, 보험, 군사 등 다양한 분야에서 진행되고 있습니다. XAI를 실생활에 접목하려면 반드시 인간과 AI 간의 상호작용을 이끌어낼 수 있는 솔루션을 찾아야 할 것입니다. 

 

미국 국방성 산하 DARPA(방위고등연구계획국)는 XAI에 필요한 해석 가능한 모델 중 하나가 그래프 기반 모델이라는 연구결과를 발표했습니다. 그래프는 데이터를 점으로 저장하고, 관계를 선으로 연결하여 데이터를 쉽고 빠르게 정리할 수 있습니다. 또, 그래프를 시각화하면 AI의 중간 처리 과정을 볼 수 있으며, 업무에도 매우 유용하게 참고할 수 있습니다. 



> 출처 
>
> https://bitnine.tistory.com/408?category=788251